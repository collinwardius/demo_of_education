#!/usr/bin/env python3
"""
Script to combine all datasets in the cleaned_scans folder and create a summary analysis.
"""

import pandas as pd
import os
from pathlib import Path
from typing import Dict, Set
import numpy as np
import re

def extract_year_from_filename(filename: str) -> int:
    """Extract the starting year from a filename like 'bi_survey1916_1918.csv'."""
    # Look for a 4-digit year in the filename
    year_match = re.search(r'(\d{4})', filename)
    if year_match:
        return int(year_match.group(1))
    # If no year found, return a high number to sort it last
    return 9999

def get_dataset_info(df: pd.DataFrame, filename: str) -> Dict:
    """Extract information about a dataset including variables and their types."""
    info = {
        'filename': filename,
        'year': extract_year_from_filename(filename),
        'shape': df.shape,
        'columns': list(df.columns),
        'dtypes': df.dtypes.to_dict(),
        'missing_values': df.isnull().sum().to_dict(),
        'unique_values': {col: df[col].nunique() for col in df.columns}
    }
    
    # Count unique colleges if college column exists
    if 'college' in df.columns:
        # Remove any null/empty values and count unique colleges
        colleges = df['college'].dropna()
        colleges = colleges[colleges.str.strip() != ''] if len(colleges) > 0 else colleges
        info['unique_colleges'] = colleges.nunique()
        info['total_college_records'] = len(colleges)
    else:
        info['unique_colleges'] = 0
        info['total_college_records'] = 0
    
    return info

def analyze_datasets(data_folder: str) -> Dict:
    """Analyze all CSV datasets in the folder, excluding generated files."""
    data_folder = Path(data_folder)
    datasets_info = {}
    all_dataframes = {}
    
    # Files to exclude (generated by this script)
    exclude_files = {
        "college_surveys_appended.csv",
        "summary_of_appended_dataset.csv"
    }
    
    print(f"Analyzing datasets in: {data_folder}")
    
    # Get all CSV files and sort them by year
    csv_files = [f for f in data_folder.glob("*.csv") if f.name not in exclude_files]
    csv_files_sorted = sorted(csv_files, key=lambda x: extract_year_from_filename(x.name))
    
    for file_path in csv_files_sorted:
        print(f"Processing: {file_path.name}")
        try:
            df = pd.read_csv(file_path)
            datasets_info[file_path.stem] = get_dataset_info(df, file_path.name)
            all_dataframes[file_path.stem] = df
        except Exception as e:
            print(f"Error reading {file_path.name}: {e}")
    
    return datasets_info, all_dataframes

def find_common_variables(datasets_info: Dict) -> Dict[str, Set[str]]:
    """Find common and unique variables across datasets."""
    all_variables = set()
    dataset_variables = {}
    
    for dataset_name, info in datasets_info.items():
        variables = set(info['columns'])
        dataset_variables[dataset_name] = variables
        all_variables.update(variables)
    
    common_vars = set.intersection(*dataset_variables.values()) if dataset_variables else set()
    unique_vars = {}
    
    for dataset_name, variables in dataset_variables.items():
        unique_vars[dataset_name] = variables - common_vars
    
    return {
        'all_variables': all_variables,
        'common_variables': common_vars,
        'unique_variables': unique_vars,
        'dataset_variables': dataset_variables
    }

def combine_datasets(all_dataframes: Dict[str, pd.DataFrame]) -> pd.DataFrame:
    """Combine all datasets into one, handling different column structures."""
    if not all_dataframes:
        return pd.DataFrame()
    
    # Get all unique columns across datasets
    all_columns = set()
    for df in all_dataframes.values():
        all_columns.update(df.columns)
    
    # Standardize all dataframes to have the same columns
    standardized_dfs = []
    for dataset_name, df in all_dataframes.items():
        df_copy = df.copy()
        df_copy['source_dataset'] = dataset_name
        
        # Add missing columns with NaN values
        for col in all_columns:
            if col not in df_copy.columns:
                df_copy[col] = np.nan
        
        # Reorder columns to match
        column_order = ['source_dataset'] + sorted([col for col in all_columns if col != 'source_dataset'])
        df_copy = df_copy[column_order]
        
        standardized_dfs.append(df_copy)
    
    # Combine all datasets
    combined_df = pd.concat(standardized_dfs, ignore_index=True)
    return combined_df

def create_summary_markdown(datasets_info: Dict, variable_analysis: Dict, output_path: str):
    """Create a comprehensive summary in markdown format."""
    
    print("Creating markdown summary...")
    
    markdown_content = []
    markdown_content.append("# Dataset Analysis Summary")
    markdown_content.append("")
    markdown_content.append(f"Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
    markdown_content.append("")
    
    # Dataset overview
    markdown_content.append("## Dataset Overview")
    markdown_content.append("")
    markdown_content.append("| Dataset | Year | Rows | Columns | Unique Colleges | File |")
    markdown_content.append("|---------|------|------|---------|----------------|------|")
    
    # Sort datasets by year for consistent ordering
    sorted_datasets = sorted(datasets_info.items(), key=lambda x: x[1]['year'])
    
    for dataset_name, info in sorted_datasets:
        rows, cols = info['shape']
        filename = info['filename']
        year = info['year'] if info['year'] != 9999 else "Unknown"
        unique_colleges = info.get('unique_colleges', 0)
        markdown_content.append(f"| {dataset_name} | {year} | {rows:,} | {cols} | {unique_colleges:,} | {filename} |")
    
    markdown_content.append("")
    
    # College statistics summary
    total_colleges_across_datasets = sum(info.get('unique_colleges', 0) for info in datasets_info.values())
    total_college_records = sum(info.get('total_college_records', 0) for info in datasets_info.values())
    
    markdown_content.append("## College Statistics Summary")
    markdown_content.append("")
    markdown_content.append(f"- **Total unique colleges across all datasets:** {total_colleges_across_datasets:,}")
    markdown_content.append(f"- **Total college-related records:** {total_college_records:,}")
    markdown_content.append("")
    
    # College statistics by year
    markdown_content.append("### Colleges by Dataset (Year Order)")
    markdown_content.append("")
    markdown_content.append("| Dataset | Year | Unique Colleges | College Records |")
    markdown_content.append("|---------|------|----------------|----------------|")
    
    for dataset_name, info in sorted_datasets:
        year = info['year'] if info['year'] != 9999 else "Unknown"
        unique_colleges = info.get('unique_colleges', 0)
        college_records = info.get('total_college_records', 0)
        markdown_content.append(f"| {dataset_name} | {year} | {unique_colleges:,} | {college_records:,} |")
    
    markdown_content.append("")
    
    # Variable analysis summary
    markdown_content.append("## Variable Analysis Summary")
    markdown_content.append("")
    markdown_content.append(f"- **Total unique variables across all datasets:** {len(variable_analysis['all_variables'])}")
    markdown_content.append(f"- **Variables common to all datasets:** {len(variable_analysis['common_variables'])}")
    markdown_content.append("")
    
    # Common variables
    if variable_analysis['common_variables']:
        markdown_content.append("### Common Variables (present in all datasets)")
        markdown_content.append("")
        for var in sorted(variable_analysis['common_variables']):
            markdown_content.append(f"- `{var}`")
        markdown_content.append("")
    
    # Unique variables by dataset
    markdown_content.append("### Dataset-Specific Variables")
    markdown_content.append("")
    
    # Sort datasets by year for unique variables section
    for dataset_name, info in sorted_datasets:
        unique_vars = variable_analysis['unique_variables'].get(dataset_name, set())
        if unique_vars:
            year = info['year'] if info['year'] != 9999 else "Unknown"
            markdown_content.append(f"#### {dataset_name} ({year})")
            for var in sorted(unique_vars):
                markdown_content.append(f"- `{var}`")
            markdown_content.append("")
    
    # Variable details for each dataset
    markdown_content.append("## Detailed Variable Information")
    markdown_content.append("")
    
    # Sort datasets by year for detailed variable information
    for dataset_name, info in sorted_datasets:
        year = info['year'] if info['year'] != 9999 else "Unknown"
        markdown_content.append(f"### {dataset_name} ({year})")
        markdown_content.append("")
        markdown_content.append("| Variable | Data Type | Missing Values | Unique Values |")
        markdown_content.append("|----------|-----------|----------------|---------------|")
        
        for col in info['columns']:
            dtype = str(info['dtypes'][col])
            missing = info['missing_values'][col]
            unique = info['unique_values'][col]
            markdown_content.append(f"| `{col}` | {dtype} | {missing:,} | {unique:,} |")
        
        markdown_content.append("")
    
    # Write to file
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(markdown_content))
    
    print(f"Summary markdown saved to: {output_path}")
    
    return markdown_content

def main():
    """Main function to run the analysis and combination."""
    # Define paths
    data_folder = "/Users/cjwardius/Library/CloudStorage/OneDrive-UCSanDiego/demo of education/data/cleaned_scans"
    output_folder = "/Users/cjwardius/Library/CloudStorage/OneDrive-UCSanDiego/demo of education/data/cleaned_scans"
    
    print("Starting dataset analysis and combination...")
    
    # Analyze datasets
    datasets_info, all_dataframes = analyze_datasets(data_folder)
    
    if not datasets_info:
        print("No datasets found!")
        return
    
    print(f"Found {len(datasets_info)} datasets:")
    for name in datasets_info.keys():
        print(f"  - {name}")
    
    # Analyze variables
    variable_analysis = find_common_variables(datasets_info)
    
    print(f"\nVariable Analysis:")
    print(f"  Total unique variables: {len(variable_analysis['all_variables'])}")
    print(f"  Common variables: {len(variable_analysis['common_variables'])}")
    print(f"  Common variables: {list(variable_analysis['common_variables'])}")
    
    # Combine datasets
    print("\nCombining datasets...")
    combined_df = combine_datasets(all_dataframes)
    
    if not combined_df.empty:
        # Save combined dataset
        combined_output_path = os.path.join(output_folder, "college_surveys_appended.csv")
        combined_df.to_csv(combined_output_path, index=False)
        print(f"Combined dataset saved to: {combined_output_path}")
        print(f"Combined dataset shape: {combined_df.shape}")
        
        # Create summary markdown
        summary_output_path = os.path.join(output_folder, "summary_of_appended_dataset.md")
        summary_content = create_summary_markdown(datasets_info, variable_analysis, summary_output_path)
        
        print(f"\nSummary statistics:")
        print(f"  Total records in combined dataset: {len(combined_df)}")
        print(f"  Total columns in combined dataset: {len(combined_df.columns)}")
        print(f"  Summary markdown lines: {len(summary_content)}")
        
    else:
        print("No data to combine!")

if __name__ == "__main__":
    main()